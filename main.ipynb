{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/mantis.csv', 'row': 0, 'DR#': '25120', 'Problem Summary': 'Simulator Freeze - October 16th 2024', 'Problem Description': 'Simulator froze unexpectedly second of two LORP runs this morning.\\r\\nFollowing indications of ICCM datalink failure, simulator board controls and ovation were unresponsive for approximately 2 minutes while simulator status indicated \"Running\" on IS station. After which, the simulator continued to work as normal', 'Notes & Resolution': '@ppham 01/14/2025\\nFreeze was not reproducible during further validation and did not occur during actual exam run. Closing DR\\n=-=\\n@jclark 10/17/2024\\nIf this issue emerges again on a future validation run of the same scenario, please collect the ICs and data needed for troubleshooting.\\n'}, page_content='Problem Summary: Simulator Freeze - October 16th 2024'),\n",
       " Document(metadata={'source': 'data/mantis.csv', 'row': 1, 'DR#': '25119', 'Problem Summary': 'Thor Abort - October 16th 2024', 'Problem Description': 'THOR abort occurred during the first of two LORP runs this morning.\\r\\nAdditionally, further into the run following indications of ICCM datalink failure, simulator board controls and ovation were unresponsive for approximately 2 minutes while simulator status indicated \"Running\" on IS station. After which, the simulator continued to work as normal', 'Notes & Resolution': '@ppham 01/14/2025\\nAbort was not reproducible during further validation and did not occur during actual exam run. Closing DR\\n=-=\\n@jclark 10/17/2024\\nIf this issue emerges again on a future validation run of the same scenario, please collect the ICs and data needed for troubleshooting.\\n'}, page_content='Problem Summary: Thor Abort - October 16th 2024'),\n",
       " Document(metadata={'source': 'data/mantis.csv', 'row': 2, 'DR#': '22964', 'Problem Summary': 'Convert XElectric Model to ThunderElectric', 'Problem Description': 'Farley wants to build the switchyard and/or convert the existing 4160V XElectric model to ThunderElectric.  The intent is to use remaining maintenance hours to get as much of the project setup and graphics drawn as much as possible, knowing the actual model completion and testing may not be finished under the current agreement.  \\r\\n\\r\\nThe number of hours remaining and more detailed discussion on scope with CORYS will determine the order to proceed.', 'Notes & Resolution': \"@jclark 01/13/2025\\nPer our last telecon, Farley may close this request since the ThunderElectric upgrade is part of the site's 3-year plan.\\n\"}, page_content='Problem Summary: Convert XElectric Model to ThunderElectric'),\n",
       " Document(metadata={'source': 'data/mantis.csv', 'row': 3, 'DR#': '25588', 'Problem Summary': 'Panel 601 - Pressure Monitoring Unit (PMU501) LO Gross Fail lights', 'Problem Description': 'For the Pressure Monitoring Unit on panel 601, I was able to assign ZLOADB21PMU501(1-23)  based on the comment in the code:\\r\\n\\r\\nZLOADB21PMU501(P1) THRU ZLOADB21PMU501(P11) ARE TRIP LIGHTS.\\r\\nZLOADB21PMU501(P12) THRU ZLOADB21PMU501(P22) ARE GROSS FAIL LIGHTS\\r\\nZLOADB21PMU501(P23) IS CALIBRATION LIGHT.\\r\\n\\r\\nIt looks like there are two TRIP lights, one under the meter, and one above the meter. Unless the top TRIP light is actually the gross fail light, I am unsure what the gross fail light looks like. Is this something that is functional on your simulator?', 'Notes & Resolution': '@mmansour 01/14/2025\\nYes it is. Oops.\\n=-=\\n@pkilgore 01/13/2025\\nIs this a duplicate of DR 25468?\\n'}, page_content='Problem Summary: Panel 601 - Pressure Monitoring Unit (PMU501) LO Gross Fail lights'),\n",
       " Document(metadata={'source': 'data/mantis.csv', 'row': 4, 'DR#': '25566', 'Problem Summary': 'THOR Abort Caused by Loss of Core Cooling', 'Problem Description': 'We recently experienced an Executive crash while developing a scenario. This issue is repeatable by using IC 67 that has been uploaded to the FTP site. The issue should occur after running this IC for about a minute.\\r\\n\\r\\nThis issue occurred after restoring cooling water to the reactor after a LOCA. We are not currently using MELCOR.\\r\\n\\r\\nPlease review this crash to determine cause and recommend solution to prevent issue.', 'Notes & Resolution': '@Mlegreve 01/13/2025\\nNo further action needed at this time.\\n=-=\\n@rsanders 01/13/2025\\nReviewed conditions associated with DR. Reactor Vessel has almost been depleted of water. Maximum Cladding temperature has reached run away oxidation temperature (i.e., greater than 1850 F).  Plant conditions have progressed from EOP to SAMG conditions in which core degradation is expected.  Suggest revising the scenario or implementing MELCOR if modeling of this condition is required.\\n'}, page_content='Problem Summary: THOR Abort Caused by Loss of Core Cooling')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "def encode_csv(path, model, chunk_size=1000, chunk_overlap=0):\n",
    "    # Load PDF documents\n",
    "    loader = CSVLoader(file_path=path,\n",
    "        csv_args={\n",
    "        'delimiter': ',',\n",
    "        'quotechar': '\"',\n",
    "        'fieldnames': ['DR#', 'Problem Summary', 'Problem Description', 'Notes & Resolution']},\n",
    "        metadata_columns=['DR#', 'Problem Summary', 'Problem Description', 'Notes & Resolution'],\n",
    "        # content_columns=['Problem Summary', 'Problem Description', 'Notes & Resolution'],\n",
    "        content_columns=['Problem Summary'],\n",
    "        encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "encode_csv(\"data/mantis.csv\", \"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 169597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   9%|▉         | 470/5300 [00:20<03:30, 22.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPassages:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(passages))\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m corpus_embeddings = \u001b[43mbi_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Projects\\AI\\sentence-transformers\\sentence_transformers\\SentenceTransformer.py:619\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    610\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m    611\u001b[39m             features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m    612\u001b[39m                 (\n\u001b[32m    613\u001b[39m                     features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    616\u001b[39m                 -\u001b[32m1\u001b[39m,\n\u001b[32m    617\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m features.update(extra_features)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Projects\\AI\\sentence-transformers\\sentence_transformers\\util.py:1083\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "from tqdm.autonotebook import tqdm\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169597 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169597/169597 [00:02<00:00, 68928.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# We also compare the results to lexical search (keyword search). Here, we use \n",
    "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# We lower case our text and remove stop-words from indexing\n",
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "tokenized_corpus = []\n",
    "for passage in tqdm(passages):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will search all wikipedia articles for passages that\n",
    "# answer the query\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    ##### BM25 search (lexical search) #####\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
    "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # print(\"Top-3 lexical search (BM25) hits\")\n",
    "    # for hit in bm25_hits[0:3]:\n",
    "    #     print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    ##### Semantic Search #####\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    question_embedding = question_embedding.cuda()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    ##### Re-Ranking #####\n",
    "    # Now, score all retrieved passages with the cross_encoder\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    # Sort results by the cross-encoder scores\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    # Output of top-5 hits from bi-encoder\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    # print(\"Top-3 Bi-Encoder Retrieval hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "    # for hit in hits[0:3]:\n",
    "    #     print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "\n",
    "    # Output of top-5 hits from re-ranker\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    # print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    result = list()\n",
    "    for hit in hits[0:3]:\n",
    "        # print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
    "        result.append(passages[hit['corpus_id']].replace(\"\\n\", \" \"))\n",
    "\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: What is the capital of the United States?\n",
      "According to the provided data, the capital of the United States is not explicitly stated as a single city or location. However, it can be inferred that Washington, D.C. (also known as simply Washington) serves as the political center and is the official home of many major national government offices, including the President of the USA.\n",
      "\n",
      "Furthermore, the text mentions the United States Capitol, which is located in Washington, D.C., suggesting a strong connection between the capital city and the legislative branch of the federal government. It can be concluded that Washington, D.C. (or simply Washington) is often considered as the capital of the United States.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# search(query = \"What is the capital of the United States?\")\n",
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "query = \"What is the capital of the United States?\"\n",
    "data = search(query=query)\n",
    "\n",
    "output = ollama.generate(\n",
    "  model=\"llama3.2\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {query}\"\n",
    ")\n",
    "\n",
    "print(output['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the capital of the United States?\"\n",
    "data = search(query=query)\n",
    "\n",
    "output = ollama.generate(\n",
    "  model=\"llama3.2\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {query}\"\n",
    ")\n",
    "\n",
    "print(output['response'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
